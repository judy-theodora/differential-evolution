{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b9e695",
   "metadata": {},
   "source": [
    "# ECMM423 CA2: Search & Optimisation\n",
    "An implementation of Differential Evolution (specifically DE/rand/1/bin).  \n",
    "To use this Notebook, execute all the code blocks in the following sections:  \n",
    "+ [The Search Algorithm](#search)\n",
    "+ [Random Search & Stochastic Gradient Descent](#rssgd)\n",
    "+ [Problem Instances](#prob)  \n",
    "\n",
    "In the [Parameter Tuning](#param) section, the tuning of the DE parameters for each problem is conducted. Note: execution of the grid search used in parameter tuning takes a long time.  \n",
    "The [Experiments](#exp) section contains the code for conducting the experiments for each problem on the three algorithms.  \n",
    "Finally, [Analysis](#ana) contains the results of the experiments with graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb14c1eb",
   "metadata": {},
   "source": [
    "<a id='search'></a>\n",
    "# The Search Algorithm\n",
    "Run the cells below to define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb386f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from itertools import product\n",
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4264d8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_param(NP,f,CR,F,max_gen,d,obj_bounds):\n",
    "    \"\"\"\n",
    "    Checks that the parameters of the differential_evolution()\n",
    "    function are valid, raising an exception if not.\n",
    "    \"\"\"\n",
    "    # check that the objective function is valid\n",
    "    if check_obj_func(f, d) == -1:\n",
    "        raise Exception(f\"Objective function must be able to handle {d} dimensional vectors\")\n",
    "    \n",
    "    if (type(NP) is not int) or (NP<4):\n",
    "        raise Exception(\"NP must be an integer greater than or equal to 4.\")\n",
    "    if (type(CR) not in [float, np.float64]) or (CR<0 or CR>1):\n",
    "        raise Exception(\"CR must be a float in the range [0,1].\")\n",
    "    if (type(F) not in [float, np.float64]) or (F<=0 or F>2):\n",
    "        raise Exception(\"F must be a float in the range (0,2].\")\n",
    "    if (type(max_gen) is not int) or (max_gen<=0):\n",
    "        raise Exception(\"max_gen must be a positive integer.\")\n",
    "    if (type(d) is not int) or (d<=0):\n",
    "        raise Exception(\"d must be a positive integer.\")\n",
    "    if len(obj_bounds) != 2:\n",
    "        raise Exception(\"obj_bounds must be a list-like object of length two.\")\n",
    "    if obj_bounds[0]>=obj_bounds[1]:\n",
    "        raise Exception(\"The upper bound must be greater than the lower bound.\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c36458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_obj_func(f: Callable, d: int):\n",
    "    x = init_population(1,d)[0]\n",
    "    try:\n",
    "        f(x)\n",
    "    except:\n",
    "        print(f\"Objective function `{f.__name__}` not valid for handling d={d} dimensional vectors.\")\n",
    "        return -1\n",
    "    else:\n",
    "        return 0   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a33db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialise target vectors:\n",
    "# initialise a population of size pop_size of random vectors of dimension D\n",
    "# \n",
    "def init_population(NP: int, d: int, lb: float = 0, ub: float = 1):\n",
    "    \"\"\"\n",
    "    Initialise a target population of d-dimensional vectors.\n",
    "    Defaults for the vector values to be generated in the range [0,1).\n",
    "    \n",
    "    Arguments:\n",
    "    NP -- the size of the population\n",
    "    d  -- the dimension of each vector\n",
    "    lb -- lower bound of the vector values\n",
    "    up -- upper bound of the vector values\n",
    "    \n",
    "    Returns:\n",
    "    an array of length NP of d-dimensional real-valued vectors in the\n",
    "    range [lb,ub)\n",
    "    \"\"\"\n",
    "    return np.random.rand(NP,d)*(ub-lb)+lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57197a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_rand_idx(i: int, NP: int):\n",
    "    \"\"\"\n",
    "    Picks three integers from a uniform distribution that are\n",
    "    different from each other, and from the running index i.\n",
    "    \n",
    "    Arguments:\n",
    "    i -- the running index of the member of the population being mutated\n",
    "    NP -- the population size to choose integers in the range of\n",
    "    \n",
    "    Returns:\n",
    "    three random integers in the range [0,NP) that are distinct\n",
    "    and not equal to i\n",
    "    \"\"\"\n",
    "    r1,r2,r3 = np.random.randint(0,NP,3)\n",
    "    while (r1==i): r1 = np.random.randint(0,NP)\n",
    "    while (r2==r1) or (r2==i): r2 = np.random.randint(0,NP)\n",
    "    while (r3==r2) or (r3==r1) or (r3==i): r3 = np.random.randint(0,NP)\n",
    "    return r1,r2,r3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to return a mutated vector for a particular position in the vector\n",
    "def mutate_at_pos(x: np.ndarray, F: float, i: int, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Mutates a vector by picking three distinct random vectors and \n",
    "    applying a difference mutation at a specific position.\n",
    "    \n",
    "    Arguments:\n",
    "    x -- the current population\n",
    "    F -- the differential weight\n",
    "    i -- the current index in the population iteration\n",
    "    k -- the current index of vector iteration\n",
    "    \n",
    "    Returns:\n",
    "    the mutated value for use at position k in the trial vector\"\"\"\n",
    "    r1,r2,r3 = pick_rand_idx(i,len(x))\n",
    "    mut = x[r1][k]+F*(x[r2][k]-x[r3][k])\n",
    "    return mut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd074f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def differential_evolution(NP: int, f: Callable[[np.ndarray],float], CR: float = 0.9, F: float = 0.8,\n",
    "                           max_gen: int = 100, d: int = 2, obj_bounds: tuple = (-1,1) ):\n",
    "    \"\"\"\n",
    "    Perform the differential evolution algorithm.\n",
    "    \n",
    "    Parameters:\n",
    "    NP -- population size\n",
    "    f -- the objective function\n",
    "    CR -- crossover rate\n",
    "    F  -- differential weight/mutation constant\n",
    "    max_gen -- maximum number of generations\n",
    "    d -- number of dimensions of the objective function\n",
    "    obj_bounds -- bounds of the objective function (defaults in range [-1,1])\n",
    "                  i.e. the range to search within\n",
    "    \n",
    "    Returns:\n",
    "    the fittest solution after max_gen generations.\"\"\"\n",
    "    #check_param(NP,f,CR,F,max_gen,d,obj_bounds) # check all parameters are valid\n",
    "    \n",
    "    # initialise arrays\n",
    "    x = init_population(NP,d,*obj_bounds) # randomised initial population\n",
    "    u = np.zeros((NP,d)) # array of trial vectors\n",
    "    x_g = np.zeros((NP,d)) # array for storing generation G+1\n",
    "    \n",
    "    n_gen = 0\n",
    "    while (n_gen<max_gen): # or convergence condition?\n",
    "        for i in range(NP):\n",
    "            # generate random numbers for determining mutation/crossover\n",
    "            rnbr = np.random.randint(d) # random idx 0,...,d-1\n",
    "            randb = np.random.random(d) # random number in [0,1)\n",
    "            \n",
    "            # iterate through each dimension of target vector\n",
    "            # and mutate its value in that dimension\n",
    "            for k in range(d): \n",
    "                if (k==rnbr) or (randb[k]<CR):\n",
    "                    u[i][k] = mutate_at_pos(x,F,i,k)\n",
    "                else:\n",
    "                    u[i][k] = x[i][k]\n",
    "            \n",
    "            # selection: if u[i] fitter than x[i] then use it in next generation\n",
    "            if f(u[i]) < f(x[i]): x_g[i] = u[i]\n",
    "            else: x_g[i] = x[i]\n",
    "        x = x_g\n",
    "        n_gen+=1\n",
    "    # determine fitness of every member in population and return the best solution\n",
    "    fitnesses = np.empty(NP)\n",
    "    for i, cand in enumerate(x):\n",
    "        fitnesses[i] = f(cand)\n",
    "    idx_best = np.argmin(fitnesses)\n",
    "    return x[idx_best]\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffa4f4b",
   "metadata": {},
   "source": [
    "<a id='rssgd'></a>\n",
    "# Random Search & Stochastic Gradient Descent\n",
    "Random search and a stochastic gradient descent implementation. Run cells to define functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9ea032",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_search(NP: int, f: Callable[[np.ndarray],float],max_gen: int = 100, d: int = 2, obj_bounds: tuple = (-1,1)):\n",
    "    x = init_population(NP,d,*obj_bounds) # randomised initial population\n",
    "    best_sol = x[0]\n",
    "    best_score = f(x[0])\n",
    "    n_gen = 0\n",
    "    while n_gen < max_gen:\n",
    "        # initialise new population each time\n",
    "        x = init_population(NP,d,*obj_bounds)\n",
    "        for v in x:\n",
    "            score = f(v)\n",
    "            if score < best_score:\n",
    "                best_score = score\n",
    "                best_sol = v\n",
    "        n_gen+=1\n",
    "    return best_sol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91adf256",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(v, f): #approximate gradient of the function at v\n",
    "    d = len(v)\n",
    "    e = 0.0000001 # small e for derivative\n",
    "    fv = f(v)\n",
    "    ret_gradient = np.zeros(d)\n",
    "    loc_gradient = 0\n",
    "    for i in range(d):\n",
    "        e_v = np.zeros(d)\n",
    "        e_v[i] = e\n",
    "        loc_gradient = (f(v+e_v)-fv)/e\n",
    "        ret_gradient[i] = loc_gradient\n",
    "    return ret_gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ba2774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(NP: int, f: Callable[[np.ndarray],float], learning_rate: float,\n",
    "                 max_gen: int = 100, d: int = 2, obj_bounds: tuple = (-1,1)):\n",
    "    \n",
    "    x = init_population(NP,d,*obj_bounds) # randomised initial population\n",
    "    n_gen = 0\n",
    "    while (n_gen<max_gen):\n",
    "        for v in x: # iterate over vector population\n",
    "            diff = grad(v, f)*learning_rate\n",
    "            v -= diff\n",
    "        n_gen+=1\n",
    "    \n",
    "    best_score = None\n",
    "    best_sol = None\n",
    "    for v in x:\n",
    "        score = f(v)\n",
    "        if best_score is None or score<best_score:\n",
    "            best_score = score\n",
    "            best_sol = v\n",
    "    return best_sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd23eeb",
   "metadata": {},
   "source": [
    "<a id='prob'></a>\n",
    "# Problem Instances\n",
    "Simply execute these blocks to define the problem functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16df928",
   "metadata": {},
   "source": [
    "## Ackley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09969bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi in [-32.768, 32.768], minimum f(0,0,...,0) = 0\n",
    "def ackley(x: np.ndarray):\n",
    "    a,b,c,d = 20, 0.2, 2*np.pi, len(x)\n",
    "    ack = -a*np.exp(-b*np.sqrt( 1/d*np.dot(x,x) )) - \\\n",
    "            np.exp( 1/d*np.sum(np.cos(c*x)) ) + a + np.e\n",
    "    return ack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da391e",
   "metadata": {},
   "source": [
    "## Rastrigin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2158e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi in [-5.12, 5.12], minimum f(0,0,...,0) = 0\n",
    "def rastrigin(x: np.ndarray):\n",
    "    A = 10\n",
    "    return A*len(x) + np.sum(x*x-A*np.cos(2*np.pi*x))   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96870cf",
   "metadata": {},
   "source": [
    "## Griewank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe88e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi in [-600, 600], minimum f(0,0,...,0) = 0\n",
    "def griewank(x: np.ndarray):\n",
    "    return 1+1/4000*np.sum(x*x)-np.product( [np.cos(xi/np.sqrt(i)) for i,xi in enumerate(x,start=1)] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430f1751",
   "metadata": {},
   "source": [
    "## Rosenbrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e820564e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi in [-5, 10] OR [-2.048, 2.048], minimum f(1,1,...,1) = 0\n",
    "def rosenbrock(x: np.ndarray):\n",
    "    d = len(x)\n",
    "    ros = 0\n",
    "    for i in range(d-1):\n",
    "        ros += 100*(x[i+1]-x[i]*x[i])**2+(1-x[i])**2\n",
    "    return ros"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548f40c6",
   "metadata": {},
   "source": [
    "## Easom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215a79e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xi in [-100, 100], 2 Dimensional, minimum f(pi,pi) = -1\n",
    "def easom(x: np.ndarray):\n",
    "    x1, x2 = x\n",
    "    return -(np.cos(x1)*np.cos(x2))*np.exp(-(x1-np.pi)**2-(x2-np.pi)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1aa705",
   "metadata": {},
   "source": [
    "# Parameter Tuning\n",
    "In this section a grid search is applied to various problem configurations (F1-F9). The search range for F,CR parameters can be altered in the first code cell. The problem configurations can be altered in the second, along with the allowed generations and total population size. (note: the execution of the main loop can take a long time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c1342c2",
   "metadata": {},
   "source": [
    "### Tuning DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef6c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "F_range = np.linspace(0.05,1.2,21)\n",
    "CR_range = np.linspace(0.05,1.0,20)\n",
    "params_ = {'F': F_range, 'CR': CR_range}\n",
    "\n",
    "search_space = np.array(list(product(*params_.values())))\n",
    "print(len(search_space), 'parameter combinations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828ba0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_functions = [easom,ackley,ackley,rosenbrock,rosenbrock,rastrigin,rastrigin,griewank,griewank]\n",
    "dimensions = [2,15,30,15,30,15,30,15,30]\n",
    "obj_bounds = [(-100,100),(-32.768,32.768),(-32.768,32.768),(-2.048,2.048),(-2.048,2.048),(-5.12,5.12),(-5.12,5.12),(-600,600),(-600,600)]\n",
    "max_gen = 150\n",
    "NP = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a0738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESET DICTIONARIES\n",
    "best_params = {}\n",
    "best_score = {}\n",
    "all_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5265d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for i in range(9):\n",
    "    obj_f = obj_functions[i]\n",
    "    d = dimensions[i]\n",
    "    bounds = obj_bounds[i]\n",
    "    print(f'Optimising F{i+1} ({obj_f.__name__}, {d}-dim)')\n",
    "    \n",
    "    best_params[i] = None\n",
    "    best_score[i] = None\n",
    "    all_scores[i] = []\n",
    "    for params in search_space:\n",
    "        F_, CR_ = params\n",
    "        sol = differential_evolution(NP=NP, f=obj_f, F=F_,CR=CR_,\n",
    "                                  obj_bounds=bounds, max_gen=max_gen, d=d)\n",
    "        score = obj_f(sol)\n",
    "        all_scores[i].append(score)\n",
    "        if best_score[i] is None or score < best_score[i]:\n",
    "            best_score[i] = score\n",
    "            best_params[i] = params\n",
    "    print(f'Best params for F{i+1}: F,CR={best_params[i]} for a score of {best_score[i]}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a309c3b2",
   "metadata": {},
   "source": [
    "<a id='exp'></a>\n",
    "# Experimental Comparison\n",
    "...with random search and a stochastic hill-climber.  \n",
    "The first code cell contains the optimum hyperparameters for DE, found from the parameter tuning section. The second code block performs the experiments, and averages the scores at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118487f",
   "metadata": {},
   "outputs": [],
   "source": [
    "de_params = {0: np.array([0.1075, 0.15  ]),\n",
    " 1: np.array([0.28, 0.75]),\n",
    " 2: np.array([0.28, 0.95]),\n",
    " 3: np.array([0.395, 0.85 ]),\n",
    " 4: np.array([0.28, 0.7 ]),\n",
    " 5: np.array([0.1075, 0.1   ]),\n",
    " 6: np.array([0.1075, 0.35  ]),\n",
    " 7: np.array([0.28, 0.8 ]),\n",
    " 8: np.array([0.28, 0.95]),}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95294dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_functions = [easom,ackley,ackley,rosenbrock,rosenbrock,rastrigin,rastrigin,griewank,griewank]\n",
    "dimensions = [2,15,30,15,30,15,30,15,30]\n",
    "obj_bounds = [(-100,100),(-32.768,32.768),(-32.768,32.768),(-2.048,2.048),(-2.048,2.048),(-5.12,5.12),(-5.12,5.12),(-600,600),(-600,600)]\n",
    "NP = 30\n",
    "total_iterations = 10\n",
    "scores = {}\n",
    "\n",
    "for i in de_params:\n",
    "    print(i)\n",
    "    scores[i] = {'de':[],'rs':[],'sgd':[]}\n",
    "    for n in range(total_iterations):\n",
    "        F_, CR_ = de_params[i]\n",
    "        obj_f = obj_functions[i]\n",
    "        d = dimensions[i]\n",
    "        bounds = obj_bounds[i]\n",
    "\n",
    "        de_sol = differential_evolution(NP=NP, f=obj_f, F=F_,CR=CR_,\n",
    "                                        obj_bounds=bounds, max_gen=333, d=d)\n",
    "        scores[i]['de'].append( obj_f(de_sol) )\n",
    "        \n",
    "        rs_sol = rand_search(NP=NP, f=obj_f, obj_bounds=bounds, max_gen=666, d=d)\n",
    "        scores[i]['rs'].append( obj_f(rs_sol) )\n",
    "        \n",
    "        sgd_sol = grad_descent(NP=NP, f=obj_f, learning_rate=0.005,\n",
    "                               obj_bounds=bounds, max_gen=333, d=d)\n",
    "        scores[i]['sgd'].append( obj_f(sgd_sol) )\n",
    "\n",
    "# calculate the mean of each algorithm's score\n",
    "avg_scores = {}\n",
    "for i in scores:\n",
    "    avg_scores[i] = {}\n",
    "    avg_scores[i]['de'] = np.mean(scores[i]['de'])\n",
    "    avg_scores[i]['rs'] = np.mean(scores[i]['rs'])\n",
    "    avg_scores[i]['sgd'] = np.mean(scores[i]['sgd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0762218e",
   "metadata": {},
   "source": [
    "<a id='ana'></a>\n",
    "# Analysis of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f857289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(avg_scores).rename({i: f'F{i+1}' for i in range(9)},axis=1).T\n",
    "print(res.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "741724fb",
   "metadata": {},
   "source": [
    "As apparent in the table above, DE far outperforms the other two algorithms, converging close to the required values in most cases, and in the cases where it doesn't reach a very optimal solution it still gets a lot closer than the other two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407f140e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
